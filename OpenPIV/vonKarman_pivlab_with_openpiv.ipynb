{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare the OpenPIV Python with PIVLab\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Analysis of the Karman images\n",
    "final int area 6 pixels and 50% overlap, \n",
    "vector validation is allowed, but no smoothing after the last correlation. \n",
    "Only the circle in the middle must be masked, not the shadows.\n",
    "\n",
    "Then we can compare the vorticity maps (color bar scale of uncalibrated data -0.3 1/frame until +0.3 1/frame, \n",
    "color map preferably \"parula\", but \"jet\" is also ok). That might give an idea about the \"quality\"...?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FFT window deformation\n",
    "Pass1: 64x64 px with 50% overlap\n",
    "Pass2: 32x32 px with 50% overlap\n",
    "Pass3: 16x16 px with 50% overlap\n",
    "Pass4: 6x6 px with 50% overlap\n",
    "Gauss2x3-point subpixel estimator\n",
    "Correlation quality: Extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -m -p numpy,openpiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpiv import windef\n",
    "from openpiv.windef import Settings\n",
    "from openpiv import tools, scaling, validation, filters, preprocess\n",
    "from openpiv.pyprocess import extended_search_area_piv, get_field_shape, get_coordinates\n",
    "from openpiv import smoothn\n",
    "from openpiv.preprocess import mask_coordinates\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 6.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings()\n",
    "\n",
    "# 'Data related settings'\n",
    "# Folder with the images to process\n",
    "settings.filepath_images = '../data/'\n",
    "# Folder for the outputs\n",
    "settings.save_path = '../OpenPIV_results/'\n",
    "# Root name of the output Folder for Result Files\n",
    "settings.save_folder_suffix = 'Test_1'\n",
    "# Format and Image Sequence\n",
    "settings.frame_pattern_a = 'karman_16Hz_000_A.jpg'\n",
    "settings.frame_pattern_b = 'karman_16Hz_000_B.jpg'\n",
    "\n",
    "'Region of interest'\n",
    "# (50,300,50,300) #Region of interest: (xmin,xmax,ymin,ymax) or 'full' for full image\n",
    "settings.ROI = 'full'\n",
    "# settings.ROI = (200,400,600,850)\n",
    "\n",
    "\n",
    "\n",
    "settings.deformation_method = 'symmetric' # or 'second image'\n",
    "\n",
    "\n",
    "settings.iterations = 4  # select the number of PIV passes\n",
    "\n",
    "# add the interrogation window size for each pass. \n",
    "# For the moment, it should be a power of 2 \n",
    "settings.windowsizes=(64, 32, 16, 6)\n",
    "settings.overlap=(32, 16, 8, 3)\n",
    "\n",
    "# settings.windowsizes = (128, 64, 32, 16, 8) # if longer than n iteration the rest is ignored\n",
    "# The overlap of the interroagtion window for each pass.\n",
    "# settings.overlap = (64, 32, 16, 8, 4) # This is 50% overlap\n",
    "\n",
    "\n",
    "# Has to be a value with base two. In general window size/2 is a good choice.\n",
    "# methode used for subpixel interpolation: 'gaussian','centroid','parabolic'\n",
    "settings.subpixel_method = 'gaussian'\n",
    "\n",
    "# order of the image interpolation for the window deformation\n",
    "settings.interpolation_order = 1\n",
    "settings.scaling_factor = 1  # scaling factor pixel/meter\n",
    "settings.dt = 1  # time between to frames (in seconds)\n",
    "'Signal to noise ratio options (only for the last pass)'\n",
    "# It is possible to decide if the S/N should be computed (for the last pass) or not\n",
    "settings.extract_sig2noise = True  # 'True' or 'False' (only for the last pass)\n",
    "# method used to calculate the signal to noise ratio 'peak2peak' or 'peak2mean'\n",
    "settings.sig2noise_method = 'peak2peak'\n",
    "# select the width of the masked to masked out pixels next to the main peak\n",
    "settings.sig2noise_mask = 2\n",
    "# If extract_sig2noise==False the values in the signal to noise ratio\n",
    "# output column are set to NaN\n",
    "\n",
    "# only effecting the first pass of the interrogation the following passes\n",
    "# in the multipass will be validated\n",
    "\n",
    "'Output options'\n",
    "# Select if you want to save the plotted vectorfield: True or False\n",
    "settings.save_plot = False\n",
    "# Choose wether you want to see the vectorfield or not :True or False\n",
    "settings.show_plot = True\n",
    "settings.scale_plot = 200  # select a value to scale the quiver plot of the vectorfield\n",
    "# run the script with the given settings\n",
    "\n",
    "\n",
    "\n",
    "# 'Processing Parameters'\n",
    "settings.correlation_method='linear'  # 'circular' or 'linear'\n",
    "settings.normalized_correlation = True\n",
    "\n",
    "# 'vector validation options'\n",
    "# choose if you want to do validation of the first pass: True or False\n",
    "settings.validation_first_pass = True\n",
    "\n",
    "\n",
    "settings.filter_method = 'localmean'\n",
    "# maximum iterations performed to replace the outliers\n",
    "settings.max_filter_iteration = 10\n",
    "settings.filter_kernel_size = 3  # kernel size for the localmean method\n",
    "\n",
    "settings.replace_vectors = True\n",
    "\n",
    "settings.MinMax_U_disp = (-5, 5)\n",
    "settings.MinMax_V_disp = (-5, 5)\n",
    "\n",
    "# The second filter is based on the global STD threshold\n",
    "settings.std_threshold = 3  # threshold of the std validation\n",
    "\n",
    "# The third filter is the median test (not normalized at the moment)\n",
    "settings.median_threshold = 3  # threshold of the median validation\n",
    "# On the last iteration, an additional validation can be done based on the S/N.\n",
    "settings.median_size=1 #defines the size of the local median, it'll be 3 x 3\n",
    "\n",
    "\n",
    "settings.dynamic_masking_method = 'intensity'\n",
    "settings.dynamic_masking_threshold = 0.1\n",
    "settings.dynamic_masking_filter_size = 21\n",
    "\n",
    "# New settings for version 0.23.2c\n",
    "settings.image_mask = True\n",
    "\n",
    "# Smoothing after the first pass\n",
    "settings.smoothn=True #Enables smoothing of the displacemenet field\n",
    "settings.smoothn_p=0.5 # This is a smoothing parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windef.piv(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and crop the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_a = settings.frame_pattern_a\n",
    "file_b = settings.frame_pattern_b\n",
    "\n",
    "# \" read images into numpy arrays\"\n",
    "frame_a = tools.imread(os.path.join(settings.filepath_images, file_a))\n",
    "frame_b = tools.imread(os.path.join(settings.filepath_images, file_b))\n",
    "\n",
    "# \" crop to ROI\"\n",
    "if settings.ROI == \"full\":\n",
    "    pass\n",
    "else:\n",
    "    frame_a = frame_a[\n",
    "        settings.ROI[0]:settings.ROI[1],\n",
    "        settings.ROI[2]:settings.ROI[3]\n",
    "    ]\n",
    "    frame_b = frame_b[\n",
    "        settings.ROI[0]:settings.ROI[1],\n",
    "        settings.ROI[2]:settings.ROI[3]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_a,cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Image preprocessing'\n",
    "# 'None' for no masking, 'edges' for edges masking, 'intensity' for intensity masking\n",
    "# WARNING: This part is under development so better not to use MASKS\n",
    "\n",
    "if settings.dynamic_masking_method == \"edge\" or \"intensity\":\n",
    "    frame_a, image_mask_a = preprocess.dynamic_masking(\n",
    "        frame_a,\n",
    "        method=settings.dynamic_masking_method,\n",
    "        filter_size=settings.dynamic_masking_filter_size,\n",
    "        threshold=settings.dynamic_masking_threshold,\n",
    "    )\n",
    "    frame_b, image_mask_b = preprocess.dynamic_masking(\n",
    "        frame_b,\n",
    "        method=settings.dynamic_masking_method,\n",
    "        filter_size=settings.dynamic_masking_filter_size,\n",
    "        threshold=settings.dynamic_masking_threshold,\n",
    "    )\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(frame_a)\n",
    "ax[1].imshow(frame_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's combine the two masks if the body is slightly moving\n",
    "image_mask = np.logical_and(image_mask_a, image_mask_b)\n",
    "plt.imshow(image_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exract coordinates of the mask as a list of coordinates of a polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_coords = mask_coordinates(image_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the first pass\n",
    "\n",
    "We use typically the most robust approach: linear correlation (with zero padding)\n",
    "    and normalized correlation function (0..1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to convert the image mask to the data mask in x,y \n",
    "# coordinates, we have to either run first pass or \n",
    "# use get_coordinates\n",
    "# Since we do not know how to use the image_mask in the \n",
    "# first pass with the vectorized correlations, i.e. how to \n",
    "# save some computational time by skipping the interrogation\n",
    "# windows within the image mask, we just run the first pass\n",
    "\n",
    "\n",
    "# \"first pass\"\n",
    "x, y, u, v, sig2noise_ratio = windef.first_pass(\n",
    "    frame_a,\n",
    "    frame_b,\n",
    "    settings.windowsizes[0],\n",
    "    settings.overlap[0],\n",
    "    settings.iterations,\n",
    "    correlation_method=settings.correlation_method,\n",
    "    subpixel_method=settings.subpixel_method,\n",
    "    do_sig2noise=settings.extract_sig2noise,\n",
    "    sig2noise_method=settings.sig2noise_method,\n",
    "    sig2noise_mask=settings.sig2noise_mask,\n",
    "    normalized_correlation=settings.normalized_correlation\n",
    ")\n",
    "\n",
    "# store for the comparison of the following steps\n",
    "u0 = u.copy()\n",
    "v0 = v.copy()\n",
    "\n",
    "def status_message(u):\n",
    "    print(f\"{np.isnan(u).sum()/u.size*100:.2f}% invalid vectors out of {u.size} vectors\")\n",
    "    \n",
    "status_message(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can convert the image mask to the data mask in x,y coordinates\n",
    "\n",
    "from skimage.measure import points_in_poly\n",
    "\n",
    "# mark those points on the grid of PIV inside the mask\n",
    "xymask = points_in_poly(np.c_[y.flatten(),x.flatten()],mask_coords)\n",
    "\n",
    "plt.imshow(~image_mask,cmap=plt.cm.gray)\n",
    "plt.plot(x.flat[xymask],y.flat[xymask],'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the velocity maps for the future use in validation\n",
    "tmp = np.zeros_like(x,dtype=bool)\n",
    "tmp.flat[xymask] = True\n",
    "\n",
    "u = np.ma.masked_array(u, mask = tmp)\n",
    "v = np.ma.masked_array(v, mask = tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to remove those values for the display\n",
    "def quick_quiver():\n",
    "    \"\"\" u,v expected to have a mask \"\"\"\n",
    "    plt.quiver(x,y,u,v,sig2noise_ratio, scale=50,color='b')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect(1)\n",
    "    plt.plot(x.flat[xymask],y.flat[xymask],'rx')\n",
    "    plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_quiver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the distribution of the signal to noise ratio\n",
    "tmp = sig2noise_ratio.copy()\n",
    "tmp[tmp>1000] = 0.0  # there are some extra high values 1e7 ...\n",
    "plt.imshow(tmp)\n",
    "plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tmp.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider 5% of signoise ratio problems. \n",
    "sig2noise_threshold = np.percentile(sig2noise_ratio[sig2noise_ratio>0],(5))\n",
    "print(f\"S2N threshold is estimated as {sig2noise_threshold:.3f}\")\n",
    "\n",
    "settings.sig2noise_threshold = sig2noise_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v, mask_s2n = validation.sig2noise_val(\n",
    "            u, v, sig2noise_ratio,\n",
    "            threshold=settings.sig2noise_threshold\n",
    ")\n",
    "\n",
    "status_message(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(x,y,u,v,sig2noise_ratio)\n",
    "plt.quiver(x[mask_s2n],y[mask_s2n],u0[mask_s2n],v0[mask_s2n],color='r')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False everywhere, all passes\n",
    "outliers_mask = np.full_like(x,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(v.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Validation Parameters'\n",
    "# The validation is done at each iteration based on three filters.\n",
    "# The first filter is based on the min/max ranges. Observe that these values are defined in\n",
    "# terms of minimum and maximum displacement in pixel/frames.\n",
    "\n",
    "u, v, mask_g = validation.global_val(\n",
    "    u, v, settings.MinMax_U_disp, settings.MinMax_V_disp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_message(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(x,y,u,v,sig2noise_ratio)\n",
    "plt.quiver(x[mask_g],y[mask_g],u0[mask_g],v0[mask_g],color='r')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## also global std should take masked array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second filter is based on the global STD threshold\n",
    "settings.std_threshold = 3  # threshold of the std validation\n",
    "\n",
    "u, v, mask_s = validation.global_std(\n",
    "    u, v, std_threshold=settings.std_threshold\n",
    ")\n",
    "\n",
    "\n",
    "status_message(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(x,y,u,v,sig2noise_ratio)\n",
    "plt.quiver(x[mask_s],y[mask_s],u0[mask_s],v0[mask_s],color='r')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation.local_median_val should also take masked array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third filter is the median test (not normalized at the moment)\n",
    "settings.median_threshold = 3  # threshold of the median validation\n",
    "# On the last iteration, an additional validation can be done based on the S/N.\n",
    "settings.median_size=1 #defines the size of the local median\n",
    "\n",
    "u, v, mask_m = validation.local_median_val(\n",
    "    u,\n",
    "    v,\n",
    "    u_threshold=settings.median_threshold,\n",
    "    v_threshold=settings.median_threshold,\n",
    "    size=settings.median_size,\n",
    ")\n",
    "\n",
    "\n",
    "status_message(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(x,y,u,v,sig2noise_ratio)\n",
    "plt.quiver(x[mask_m],y[mask_m],u0[mask_m],v0[mask_m],color='r')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining masks\n",
    "outliers_mask = mask_g + mask_m + mask_s + mask_s2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(x,y,u,v,sig2noise_ratio)\n",
    "plt.quiver(x[outliers_mask],y[outliers_mask],u0[outliers_mask],v0[outliers_mask],color='r')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_message(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"filter to replace the values that where marked by the validation\"\n",
    "# if settings.iterations > 1:\n",
    "\n",
    "\n",
    "u, v = filters.replace_outliers(\n",
    "    u,\n",
    "    v,\n",
    "    method=settings.filter_method,\n",
    "    max_iter=settings.max_filter_iteration,\n",
    "    kernel_size=settings.filter_kernel_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the velocity maps\n",
    "tmp = np.zeros_like(x,dtype=bool)\n",
    "tmp.flat[xymask] = 1\n",
    "\n",
    "u = np.ma.masked_array(u, mask = tmp)\n",
    "v = np.ma.masked_array(v, mask = tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_quiver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing after the first pass\n",
    "settings.smoothn=True #Enables smoothing of the displacemenet field\n",
    "settings.smoothn_p=0.5 # This is a smoothing parameter\n",
    "\n",
    "u, dummy_u1, dummy_u2, dummy_u3 = smoothn.smoothn(\n",
    "    u, s=settings.smoothn_p\n",
    ")\n",
    "v, dummy_v1, dummy_v2, dummy_v3 = smoothn.smoothn(\n",
    "    v, s=settings.smoothn_p\n",
    ")\n",
    "\n",
    "# mask the velocity maps\n",
    "tmp = np.zeros_like(x,dtype=bool)\n",
    "tmp.flat[xymask] = 1\n",
    "\n",
    "u = np.ma.masked_array(u, mask = tmp)\n",
    "v = np.ma.masked_array(v, mask = tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.quiver(x,y,u0,v0,color='r',scale=30,alpha=0.5)\n",
    "plt.quiver(x,y,u,v,sig2noise_ratio,scale=30)\n",
    "plt.plot(x.flat[xymask],y.flat[xymask],'ro')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.gca().set_aspect(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-pass loop with window deformation, validation and smoothing\n",
    "\n",
    "**Note**: no smoothing on the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, settings.iterations): ## all other passes\n",
    "    x, y, u, v, sig2noise_ratio = windef.multipass_img_deform(\n",
    "        frame_a,\n",
    "        frame_b,\n",
    "        settings.windowsizes[i],\n",
    "        settings.overlap[i],\n",
    "        settings.iterations,\n",
    "        i,\n",
    "        x,\n",
    "        y,\n",
    "        u,\n",
    "        v,\n",
    "        correlation_method=settings.correlation_method,\n",
    "        subpixel_method=settings.subpixel_method,\n",
    "        deformation_method=settings.deformation_method,\n",
    "        do_sig2noise=settings.extract_sig2noise,\n",
    "        sig2noise_method=settings.sig2noise_method,\n",
    "        sig2noise_mask=settings.sig2noise_mask,\n",
    "        interpolation_order=settings.interpolation_order,\n",
    "        normalized_correlation=settings.normalized_correlation,\n",
    "        mask_coords=mask_coords,\n",
    "    )\n",
    "    \n",
    "    mask = u.mask\n",
    "    \n",
    "    # Now we can first validate, filter out, interpolate\n",
    "    # and then smooth the data for this pass:\n",
    "    \n",
    "    outliers_mask = np.full_like(x,False)\n",
    "    \n",
    "    if (\n",
    "        settings.extract_sig2noise is True\n",
    "        and i == settings.iterations\n",
    "        and settings.iterations != 1\n",
    "        and settings.do_sig2noise_validation is True\n",
    "    ):\n",
    "        u, v, mask_s2n = validation.sig2noise_val(\n",
    "            u, v, sig2noise_ratio, threshold=settings.sig2noise_threshold\n",
    "        )\n",
    "        outliers_mask += mask_s2n\n",
    "\n",
    "    \n",
    "    # validation using gloabl limits and local median\n",
    "    u, v, mask_g = validation.global_val(u, v, settings.MinMax_U_disp, \n",
    "                                         settings.MinMax_V_disp)\n",
    "    u, v, mask_s = validation.global_std(u, v, \n",
    "                                         std_threshold=settings.std_threshold)\n",
    "    u, v, mask_m = validation.local_median_val(u, v,\n",
    "                        u_threshold=settings.median_threshold,\n",
    "                        v_threshold=settings.median_threshold,\n",
    "                        size=settings.median_size,\n",
    "    )\n",
    "\n",
    "    # adding masks to add the effect of alle the validations\n",
    "    outliers_mask += (mask_g + mask_s + mask_m)\n",
    "\n",
    "    # filter to replace the values that where marked by the validation\n",
    "    # not applied at the last step \n",
    "\n",
    "    if settings.replace_vectors is True:\n",
    "        u, v = filters.replace_outliers(\n",
    "            u,\n",
    "            v,\n",
    "            method=settings.filter_method,\n",
    "            max_iter=settings.max_filter_iteration,\n",
    "            kernel_size=settings.filter_kernel_size,\n",
    "        )\n",
    "        \n",
    "    if (i != settings.iterations) and (settings.smoothn is True):\n",
    "        u, _, _, _ = smoothn.smoothn(\n",
    "                     u, s=settings.smoothn_p )\n",
    "        v, _, _, _ = smoothn.smoothn(\n",
    "                     v, s=settings.smoothn_p)\n",
    "\n",
    "    # reapply image mask just to be sure\n",
    "    if len(mask_coords) > 1:\n",
    "        u = np.ma.masked_array(u, mask=mask)\n",
    "        v = np.ma.masked_array(v, mask=mask)\n",
    "\n",
    "        \n",
    "# TODO: remove\n",
    "plt.figure()\n",
    "plt.quiver(x, y, u, v,color='r')\n",
    "# plt.quiver(x_int, y_int, u_pre, v_pre,color='b')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gca().set_aspect(1)\n",
    "plt.title('end of the loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '.'\n",
    "counter = 0\n",
    "\n",
    "# \"pixel/frame->pixel/sec\"\n",
    "u = u / settings.dt\n",
    "v = v / settings.dt\n",
    "\n",
    "# \"scales the results pixel-> meter\"\n",
    "x, y, u, v = scaling.uniform(x, y, u, v,\n",
    "                             scaling_factor=settings.scaling_factor)\n",
    "# \"save to a file\"\n",
    "tools.save(\n",
    "    x,\n",
    "    y,\n",
    "    u,\n",
    "    v,\n",
    "    sig2noise_ratio,\n",
    "    outliers_mask,\n",
    "    os.path.join(save_path, \"field_A%03d.txt\" % counter),\n",
    "    delimiter=\"\\t\",\n",
    ")\n",
    "\n",
    "# \"some other stuff that one might want to use\"\n",
    "settings.show_plot = True\n",
    "settings.save_plot = True\n",
    "\n",
    "if settings.show_plot is True or settings.save_plot is True:\n",
    "    plt.close(\"all\")\n",
    "    plt.ioff()\n",
    "    filename = os.path.join(save_path, \"Image_A%03d.png\" % counter)\n",
    "    tools.display_vector_field(\n",
    "        os.path.join(save_path, \"field_A%03d.txt\" % counter),\n",
    "        scale=settings.scale_plot,\n",
    "    )\n",
    "    if settings.save_plot is True:\n",
    "        plt.savefig(filename)\n",
    "    if settings.show_plot is True:\n",
    "        plt.show()\n",
    "\n",
    "print(\"Image Pair \" + str(counter+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = vars(settings)\n",
    "print(', '.join(\"%s: %s \\n\" % item for item in attrs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:openpiv] *",
   "language": "python",
   "name": "conda-env-openpiv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
